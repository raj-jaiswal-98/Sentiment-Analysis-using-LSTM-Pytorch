{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3228218-7951-4310-a5de-9695141732d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "import nltk\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e777e698-03a6-470f-9a02-a7053bf566f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2c32f9-8b4e-44c3-a78c-abbedc44109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, no_layers, vocab_size, hidden_dim, embedding_dim, drop_prob = 0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        \n",
    "        self.no_layers = no_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        #embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  \n",
    "        \n",
    "        #LSTM\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim, hidden_size = self.hidden_dim, num_layers = no_layers, batch_first=True)\n",
    "        \n",
    "        #dropout layers\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        #linear and Sigmoid layer\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # we just passed a batch\n",
    "        batch_size = x.size(0) # batch size -> B\n",
    "        #embed shape -> [B, max_len, embed_dim]\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        \n",
    "        # drop out and fully connected\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid \n",
    "        \n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        #reshape to batch size first\n",
    "        \n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        \n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        # create hidden state and cell state tensors with size [no_layers x batch_size x hidden_dim]\n",
    "        \n",
    "        hidden_state = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        cell_state = torch.zeros((self.no_layers, batch_size, self.hidden_dim)).to(device)\n",
    "        hidden = (hidden_state, cell_state)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddae7190-e745-429b-b6c3-311e6f22f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pickle.load(open('model.pkl', 'rb'))\n",
    "vocab = pickle.load(open('vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18b21156-2044-4ce3-ab11-4169a48e6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dd2ce56-1f86-4672-9e41-d0388e66c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'model_state.pkl'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b79e361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(1001, 64)\n",
       "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dim = 1\n",
    "model = SentimentLSTM(2, len(vocab)+1, 256, 64)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a5d7fc-a67e-4150-ba01-a463d8617e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of SentimentLSTM(\n",
       "  (embedding): Embedding(1001, 64)\n",
       "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "724565b1-336b-4db8-b3c2-66472bccf0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    # remove all characters except letters and digits\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    #remove all extra whites spaces\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    #remove digits\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356df89c-32fe-4ed2-8c2a-c559819af1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sents, seq_len):\n",
    "    features = np.zeros((len(sents), seq_len), dtype = int)\n",
    "    for i, rev in enumerate(sents):\n",
    "        if len(rev) != 0:\n",
    "            features[i, -len(rev):] = np.array(rev)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08271f0-2a7b-46f9-b537-7d801489dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() if preprocess_string(word) in vocab.keys()])\n",
    "    word_seq = np.expand_dims(word_seq, axis = 0)\n",
    "    # print(word_seq)\n",
    "    pad = torch.from_numpy(padding(word_seq, 500))\n",
    "    \n",
    "    inputs = pad.to(device)\n",
    "    batch_size = 1\n",
    "    h = model.init_hidden(batch_size)\n",
    "    output, h = model(inputs, h)\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7944bed1-bd55-4d66-b4fa-02ed0e0d4f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rajwa\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:812: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:982.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6819132566452026"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This movie is pretty good!!\"\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c632ddf8-0dad-4cc0-9419-31c96e1e91fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
